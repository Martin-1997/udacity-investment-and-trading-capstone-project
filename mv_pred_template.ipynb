{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2f8d9f-430a-48ee-b4ea-074425356596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/tepxdcepTbY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678309a8-8384-4310-89a5-d53ceabd28ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 21:45:30.721746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-28 21:45:30.721765: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import access_api as api\n",
    "from datetime import datetime, timedelta\n",
    "#Libraries that will help us extract only business days in the US.\n",
    "#Otherwise our dates would be wrong when we look back (or forward).  \n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dbf60e-602f-492d-a0b3-753266ba02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"BTC-USD\", \"AAPL\", \"GOOG\", \"GC=F\"]\n",
    "target_var = \"AAPL\"\n",
    "start_date = datetime(year = 2019, month = 11, day = 10)\n",
    "end_date = datetime(year = 2021, month = 11, day = 10)\n",
    "n_days_for_prediction=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb533485-5910-4ea5-9b5b-9c9376bb77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tickers, target_var, start_date, end_date, n_days_for_prediction):\n",
    "    df = api.get_adj_close_df(tickers, start_date, end_date)\n",
    "    df = df.dropna() # Necessary when we have ticker symbols (f.e. Bitcoin) which has a price also for weekends - we need to drop these prices because stocks do not have prices on the weekend\n",
    "    df[\"Date\"] = df.index\n",
    "    df.index = range(1, df.shape[0] + 1)\n",
    "\n",
    "    #Separate dates for future plotting\n",
    "    train_dates = pd.to_datetime(df['Date'])\n",
    "    # print(train_dates.tail(15)) #Check last few dates. \n",
    "\n",
    "    #Variables for training\n",
    "    cols = list(df)\n",
    "    cols.remove(\"Date\")\n",
    "\n",
    "    #New dataframe with only training data - 5 columns\n",
    "    df_for_training = df[cols].astype(float)\n",
    "\n",
    "    print(f\"Data columns used to build model: {cols}\") #['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "\n",
    "    target_var_x_index = list(df_for_training).index(target_var)\n",
    "    print(f\"Column to predict: {cols[target_var_x_index]} (Index: {target_var_x_index})\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "    # normalize the dataset\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(df_for_training)\n",
    "    df_for_training_scaled = scaler.transform(df_for_training)\n",
    "    print(f\"Dataset successfully scaled\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "    #In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). \n",
    "\n",
    "    #Empty lists to be populated using formatted training data\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "    n_past = 60  # Number of past days we want to use to predict the future.\n",
    "\n",
    "    #Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "    #In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "    #12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "        # trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "        # trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, target_var_x_index])\n",
    "\n",
    "\n",
    "    trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "    print(\"Train dataset was successfully created:\")\n",
    "    print(f\"\\ttrainX shape == {trainX.shape}\")\n",
    "    print(f\"\\ttrainY shape == {trainY.shape}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #In my case, trainX has a shape (12809, 14, 5). \n",
    "    #12809 because we are looking back 14 days (12823 - 14 = 12809). \n",
    "    #Remember that we cannot look back 14 days until we get to the 15th day. \n",
    "    #Also, trainY has a shape (12809, 1). Our model only predicts a single value, but \n",
    "    #it needs multiple variables (5 in my example) to make this prediction. \n",
    "    #This is why we can only predict a single day after our training, the day after where our data ends.\n",
    "    #To predict more days in future, we need all the 5 variables which we do not have. \n",
    "    #We need to predict all variables if we want to do that. \n",
    "\n",
    "    # define the Autoencoder model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(trainY.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    print(\"Model was successfully created:\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(trainX, trainY, epochs=25, batch_size=16, validation_split=0.1, verbose=1)\n",
    "    print(\"Model training successfull\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Model performance:\")\n",
    "    plt.plot(history.history['loss'], label='Training loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "    plt.legend()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "\n",
    "    #Remember that we can only predict one day in future as our model needs 5 variables\n",
    "    #as inputs for prediction. We only have all 5 variables until the last day in our dataset.\n",
    "    n_past = 1 #TODO check what that variable does\n",
    "\n",
    "    predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\n",
    "    # print(predict_period_dates)\n",
    "\n",
    "    #Make prediction\n",
    "    prediction = model.predict(trainX[-n_days_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction\n",
    "    print(\"Prediction successfully created\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #Perform inverse transformation to rescale back to original range\n",
    "    #Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "    #Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "    prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "    y_pred_future = scaler.inverse_transform(prediction_copies)[:,target_var_x_index]\n",
    "\n",
    "    # Convert timestamp to date\n",
    "    forecast_dates = []\n",
    "    for time_i in predict_period_dates:\n",
    "        forecast_dates.append(time_i.date())\n",
    "    df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), target_var :y_pred_future})\n",
    "    df_forecast['Date'] = pd.to_datetime(df_forecast['Date'])\n",
    "\n",
    "    original = df[['Date', target_var]].copy()\n",
    "    original['Date'] = pd.to_datetime(original['Date'])\n",
    "    original = original.loc[original['Date'] >= '2021-5-1']\n",
    "    \n",
    "    combined_dfs = original.append(df_forecast)\n",
    "    last_date_from_data = original[\"Date\"][original.index[-1]]\n",
    "\n",
    "    return last_date_from_data, combined_dfs, original, df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b2bf7c-3990-4405-89c8-f238f8e98bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns used to build model: ['BTC-USD', 'GC=F', 'AAPL', 'GOOG']\n",
      "Column to predict: AAPL (Index: 2)\n",
      "\n",
      "\n",
      "Dataset successfully scaled\n",
      "\n",
      "\n",
      "Train dataset was successfully created:\n",
      "\ttrainX shape == (437, 60, 4)\n",
      "\ttrainY shape == (437, 1)\n",
      "\n",
      "\n",
      "Model was successfully created:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 21:45:36.101624: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-28 21:45:36.101643: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-28 21:45:36.101664: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-ThinkPad-T490): /proc/driver/nvidia/version does not exist\n",
      "2021-11-28 21:45:36.101814: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "lstm (LSTM)                  (None, 60, 64)            17664     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,113\n",
      "Trainable params: 30,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 21:45:36.326470: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 32ms/step - loss: 0.4541 - val_loss: 0.0962\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.2093 - val_loss: 0.0571\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.0755 - val_loss: 0.0235\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0638 - val_loss: 0.0314\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.0616 - val_loss: 0.0381\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0537 - val_loss: 0.0362\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0624 - val_loss: 0.0694\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0475 - val_loss: 0.0737\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0394 - val_loss: 0.0843\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.0472 - val_loss: 0.0400\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.0415 - val_loss: 0.0434\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.0429 - val_loss: 0.0374\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0359 - val_loss: 0.0704\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0449 - val_loss: 0.0337\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 0.0402 - val_loss: 0.0585\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0414 - val_loss: 0.0331\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0345 - val_loss: 0.0423\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.0357 - val_loss: 0.0408\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0331 - val_loss: 0.0343\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.0312 - val_loss: 0.0329\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.0344 - val_loss: 0.0399\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0348 - val_loss: 0.0294\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.0313 - val_loss: 0.0350\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.0352 - val_loss: 0.0326\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.0303 - val_loss: 0.0281\n",
      "Model training successfull\n",
      "\n",
      "\n",
      "Model performance:\n",
      "\n",
      "\n",
      "Prediction successfully created\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_date_from_data, combined_dfs, original, df_forecast = get_prediction(tickers, target_var, start_date, end_date, n_days_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d77694-7512-4502-8373-0b75020ed401",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Adj Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.software/anaconda3/envs/uc-stock-price-pred/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.software/anaconda3/envs/uc-stock-price-pred/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.software/anaconda3/envs/uc-stock-price-pred/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Adj Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68434/3847155941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adj Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adj Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.software/anaconda3/envs/uc-stock-price-pred/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.software/anaconda3/envs/uc-stock-price-pred/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Adj Close'"
     ]
    }
   ],
   "source": [
    "sns.lineplot(x = original['Date'], y = original['Adj Close'])\n",
    "sns.lineplot(x = df_forecast['Date'], y = df_forecast['Adj Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08157054-4584-4300-9c2a-2c50f655fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the data in one dataset so that we can plot a continuous line\n",
    "# combined_dfs = original.append(df_forecast)\n",
    "# last_date_from_data = original[\"Date\"][original.index[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a4dc0-d188-42bb-b94b-30a00833308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.dates as mdates\n",
    "# years = mdates.YearLocator()   # every year\n",
    "# months = mdates.MonthLocator()  # every month\n",
    "# years_fmt = mdates.DateFormatter('%Y-%m') #This is a format. Will be clear in Screenshot\n",
    "# axes.xaxis.set_major_locator(months)\n",
    "# axes.xaxis.set_major_formatter(years_fmt)\n",
    "# axes.xaxis.set_minor_locator(months)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20,7))\n",
    "plt.axvline(last_date_from_data, color = \"r\")\n",
    "sns.lineplot(x = combined_dfs[\"Date\"], y = combined_dfs[target_var], label = target_var + ' price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d06cc-78ef-42ff-9ee3-a9edfdd2dd97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15cc0d53-a94d-44b6-992c-76e66e9ce237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import seaborn as sns\n",
    "# #from datetime import datetime\n",
    "\n",
    "# #Read the csv file\n",
    "# df = pd.read_csv('AAPL.csv')\n",
    "# print(df.head()) #7 columns, including the Date. \n",
    "\n",
    "# #Separate dates for future plotting\n",
    "# train_dates = pd.to_datetime(df['Date'])\n",
    "# print(train_dates.tail(15)) #Check last few dates. \n",
    "\n",
    "# #Variables for training\n",
    "# cols = list(df)[1:6]\n",
    "# #Date and volume columns are not used in training. \n",
    "# print(cols) #['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "\n",
    "# #New dataframe with only training data - 5 columns\n",
    "# df_for_training = df[cols].astype(float)\n",
    "\n",
    "# # df_for_plot=df_for_training.tail(5000)\n",
    "# # df_for_plot.plot.line()\n",
    "\n",
    "# #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# # normalize the dataset\n",
    "# scaler = StandardScaler()\n",
    "# scaler = scaler.fit(df_for_training)\n",
    "# df_for_training_scaled = scaler.transform(df_for_training)\n",
    "\n",
    "\n",
    "# #As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "# #In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). \n",
    "\n",
    "# #Empty lists to be populated using formatted training data\n",
    "# trainX = []\n",
    "# trainY = []\n",
    "\n",
    "# n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "# n_past = 60  # Number of past days we want to use to predict the future.\n",
    "\n",
    "# #Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "# #In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "# #12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "# for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "#     trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "#     trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "# trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "# print('trainX shape == {}.'.format(trainX.shape))\n",
    "# print('trainY shape == {}.'.format(trainY.shape))\n",
    "\n",
    "# #In my case, trainX has a shape (12809, 14, 5). \n",
    "# #12809 because we are looking back 14 days (12823 - 14 = 12809). \n",
    "# #Remember that we cannot look back 14 days until we get to the 15th day. \n",
    "# #Also, trainY has a shape (12809, 1). Our model only predicts a single value, but \n",
    "# #it needs multiple variables (5 in my example) to make this prediction. \n",
    "# #This is why we can only predict a single day after our training, the day after where our data ends.\n",
    "# #To predict more days in future, we need all the 5 variables which we do not have. \n",
    "# #We need to predict all variables if we want to do that. \n",
    "\n",
    "# # define the Autoencoder model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# # fit the model\n",
    "# history = model.fit(trainX, trainY, epochs=5, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Training loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "# plt.legend()\n",
    "\n",
    "# #Predicting...\n",
    "# #Libraries that will help us extract only business days in the US.\n",
    "# #Otherwise our dates would be wrong when we look back (or forward).  \n",
    "# from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "# from pandas.tseries.offsets import CustomBusinessDay\n",
    "# us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "# #Remember that we can only predict one day in future as our model needs 5 variables\n",
    "# #as inputs for prediction. We only have all 5 variables until the last day in our dataset.\n",
    "# n_past = 16\n",
    "# n_days_for_prediction=15  #let us predict past 15 days\n",
    "\n",
    "# predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\n",
    "# print(predict_period_dates)\n",
    "\n",
    "# #Make prediction\n",
    "# prediction = model.predict(trainX[-n_days_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction\n",
    "\n",
    "# #Perform inverse transformation to rescale back to original range\n",
    "# #Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "# #Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "# prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "# y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n",
    "\n",
    "\n",
    "# # Convert timestamp to date\n",
    "# forecast_dates = []\n",
    "# for time_i in predict_period_dates:\n",
    "#     forecast_dates.append(time_i.date())\n",
    "    \n",
    "# df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'Open':y_pred_future})\n",
    "# df_forecast['Date']=pd.to_datetime(df_forecast['Date'])\n",
    "\n",
    "\n",
    "# original = df[['Date', 'Open']]\n",
    "# original['Date']=pd.to_datetime(original['Date'])\n",
    "# original = original.loc[original['Date'] >= '2019-11-11']\n",
    "\n",
    "# sns.lineplot(original['Date'], original['Open'])\n",
    "# sns.lineplot(df_forecast['Date'], df_forecast['Open'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
