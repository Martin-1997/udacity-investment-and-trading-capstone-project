{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7917757-a0f6-4199-baca-a089a734781d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:54:19.453567Z",
     "iopub.status.busy": "2022-04-22T16:54:19.453300Z",
     "iopub.status.idle": "2022-04-22T16:56:31.576348Z",
     "shell.execute_reply": "2022-04-22T16:56:31.575172Z"
    },
    "id": "b7917757-a0f6-4199-baca-a089a734781d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 18:55:12.118998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-22 18:55:12.119036: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS blas_thread_init: pthread_create failed for thread 1 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 2 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 3 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 4 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 5 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 6 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n",
      "OpenBLAS blas_thread_init: pthread_create failed for thread 7 of 8: Resource temporarily unavailable\n",
      "OpenBLAS blas_thread_init: RLIMIT_NPROC 62987 current, 62987 max\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/2724222224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_db\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreturn_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtickers_to_ticker_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_all_price_data_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_formatted_train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_train_test_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define the data paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-investment-and-trading-capstone-project/models/model_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomCrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomFlip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/layers/preprocessing/image_preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_preprocessing_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras_preprocessing/image/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# flake8: noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maffine_transformations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataframe_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdirectory_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDirectoryIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras_preprocessing/image/affine_transformations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# scipy.ndimage cannot be accessed until explicitly imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mscipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scipy/ndimage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfourier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeasurements\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_nd_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scipy/special/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpecialFunctionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpecialFunctionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ufuncs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import normal libraries\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import own libraries\n",
    "from data_api.init_db import initialize_db\n",
    "from data_api.init_db import return_engine\n",
    "from data_api.db import tickers_to_ticker_ids, get_all_price_data_sets, load_formatted_train_data\n",
    "from models.model_func import create_train_test_arrays\n",
    "\n",
    "# Define the data paths\n",
    "current_dir = os.getcwd()\n",
    "database_dir = os.path.join(current_dir, \"data\")\n",
    "db_filename = \"database.db\"\n",
    "model_dir = os.path.join(current_dir, \"data/models\")\n",
    "scaler_dir = os.path.join(current_dir, \"data/scalers\")\n",
    "\n",
    "# Create the required folders, if they are not available\n",
    "if not os.path.exists(database_dir):\n",
    "    os.makedirs(database_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(scaler_dir):\n",
    "    os.makedirs(scaler_dir)\n",
    "if os.path.exists(os.path.join(database_dir, db_filename)):\n",
    "    # If the database already exits, get a connection to the database\n",
    "    engine = return_engine(database_dir, db_filename=db_filename)\n",
    "else:\n",
    "    # Otherwise, create a new database and initialize it with data\n",
    "    engine = initialize_db(database_dir,  db_filename=db_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3b22f-cf57-4077-9125-591ce8a0175c",
   "metadata": {
    "id": "8ea3b22f-cf57-4077-9125-591ce8a0175c"
   },
   "source": [
    "# Problem Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbca84b-5812-4e5a-b5c6-e6a70d2bbd6c",
   "metadata": {
    "id": "fcbca84b-5812-4e5a-b5c6-e6a70d2bbd6c"
   },
   "source": [
    "From the Yahoo Finance API, we get the 6 different values for each day. High price, low price, open price, close price, adjusted close price and volume. The 5 price data variables are all closely linked together, and only the adjusted close price reflects additional events like divident payouts or stock splits. Therefore, we only consider the adjusted close price (now referred to as \"price\") and the volume for our models. \n",
    "\n",
    "Our goal is to predict future prices based on the past prices and the past volume. We can use the price/volume data of multiple stocks to predict the price/volume data of those multiple stocks to make use of the information hidden in the dependence of the development in those different assets. The problem is, that we have indeed a long timerange of past data, but we do not know anything about future prices (yet). Therefore, we need to split the past data into small sections and consider the earlier part of that section as the past data and the later part as the future. Therefore we can create many training sets of past/future data combinations to use for our training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0eae-2b2c-4487-ba76-c2568f58f800",
   "metadata": {
    "id": "445b0eae-2b2c-4487-ba76-c2568f58f800"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b68e5-96b1-4b0a-a2db-137d8213bad4",
   "metadata": {
    "id": "654b68e5-96b1-4b0a-a2db-137d8213bad4"
   },
   "source": [
    "We need to define a useful metric or multiple metrics to evaluate the performance of our model and to compare different models with different hyperparameters. The goal of our model is to predict continious values, therefore we have a regression problem. For regression problems, we can use root mean squared error (RMSE) as a metric to optimize our model for. This metric weights larger deviations from the expected results more than lower deviations, but still has a value on the same scale as the output data.\n",
    "\n",
    "- https://keras.io/api/metrics/\n",
    "- https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae66498-3d20-4eeb-92df-c39b1d0b675b",
   "metadata": {
    "id": "cae66498-3d20-4eeb-92df-c39b1d0b675b"
   },
   "source": [
    "# EDA (Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93135ef9-7cdc-4fee-8a9b-57604ea10e19",
   "metadata": {
    "id": "93135ef9-7cdc-4fee-8a9b-57604ea10e19"
   },
   "source": [
    "When we check the data, we encounter one problem: We do not have price and volume data available for arbitrary dates for each asset. For weekends and holidays, there is usually no trading occuring on stock exchanges and therefore no price is determined. These dates without price data vary between different assets, depending on which exchanges they are listed and in which jurisdiction the exchanges are located. For some assets, like Bitcoin, there is nevertheless price data available for all days because there is less regulation about the exchanges and the cryptocurrencies can be easily traded completly digital. Additionally, there is no price data available before the asset was available. In the case of a publicly traded company, this is usually the IPO.\n",
    "To train a model with different assets, we are therefore likely facing missing values for some assets on some dates. For the first, problem, namely missing days for holidays/weekends, we have two options:\n",
    "- Drop the data for a specific date, when no price data is missing for at least one asset\n",
    "- Fill the missing prices with the average price of the days before and after\n",
    "\n",
    "In the first case, we would potentially reduce our dataset significant, in case we have many different assets from many differen exchanges/jurisdictions. Therefore we take the second option and fill up the missing prices as mentioned.\n",
    "\n",
    "To handle the second problem, namely missing asset prices because the asset was not publicly traded, it is more difficult to handle this. We can either:\n",
    "- Limit the oldest date to the date when, the last asset was available\n",
    "- Assume an average price of the asset for the time before it was available\n",
    "- Assume a price of 0 before the asset was available\n",
    "- Assume the price of the first day when the asset was available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe1e0d8-c774-4f5e-8e05-27c94a609f08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.581020Z",
     "iopub.status.busy": "2022-04-22T16:56:31.580738Z",
     "iopub.status.idle": "2022-04-22T16:56:31.596996Z",
     "shell.execute_reply": "2022-04-22T16:56:31.596184Z"
    },
    "id": "bfe1e0d8-c774-4f5e-8e05-27c94a609f08"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/4066878544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"GOOG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#, \"AAPL\", \"BTC-USD\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtickers_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers_to_ticker_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_formatted_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_timestamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "start_date=dt(2020,1,1)\n",
    "end_date=dt.today()\n",
    "tickers = [\"GOOG\"] #, \"AAPL\", \"BTC-USD\"]\n",
    "tickers_ids = tickers_to_ticker_ids(engine, tickers)\n",
    "# Load the data\n",
    "data = load_formatted_train_data(engine, tickers_ids, start_date, end_date, return_timestamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ca73d-4b8d-4ae6-86d3-13d0207e1077",
   "metadata": {
    "id": "bf2ca73d-4b8d-4ae6-86d3-13d0207e1077"
   },
   "source": [
    "# Only for  Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb-RoR4dCvop",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.600055Z",
     "iopub.status.busy": "2022-04-22T16:56:31.599787Z",
     "iopub.status.idle": "2022-04-22T16:56:31.603545Z",
     "shell.execute_reply": "2022-04-22T16:56:31.602876Z"
    },
    "id": "bb-RoR4dCvop"
   },
   "outputs": [],
   "source": [
    "# data.to_csv(\"data_big.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "G9ZfpVu7BmdS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.606694Z",
     "iopub.status.busy": "2022-04-22T16:56:31.606410Z",
     "iopub.status.idle": "2022-04-22T16:56:31.610851Z",
     "shell.execute_reply": "2022-04-22T16:56:31.610155Z"
    },
    "id": "G9ZfpVu7BmdS",
    "outputId": "e08b775e-9de0-4c6c-a3b9-52ca587d82a3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mPQbovapB3iP",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.614109Z",
     "iopub.status.busy": "2022-04-22T16:56:31.613729Z",
     "iopub.status.idle": "2022-04-22T16:56:31.616924Z",
     "shell.execute_reply": "2022-04-22T16:56:31.616237Z"
    },
    "id": "mPQbovapB3iP"
   },
   "outputs": [],
   "source": [
    "# data_small_path = \"/content/drive/MyDrive/Colab Notebooks/data_small.csv\"\n",
    "# data_tiny_path = \"/content/drive/MyDrive/Colab Notebooks/data_very_small.csv\"\n",
    "# data_big_path = \"/content/drive/MyDrive/Colab Notebooks/data_big.csv\"\n",
    "# data_tiny_path =  \"data_very_small.csv\"\n",
    "# data_small_path = \"data_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_fXSKdc3Csv6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.620270Z",
     "iopub.status.busy": "2022-04-22T16:56:31.619665Z",
     "iopub.status.idle": "2022-04-22T16:56:31.623207Z",
     "shell.execute_reply": "2022-04-22T16:56:31.622529Z"
    },
    "id": "_fXSKdc3Csv6",
    "outputId": "a4f12788-56fd-43fb-f3fa-a748ca3df1ab"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# data = pd.read_csv(data_small_path)\n",
    "# data.head()\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8mQrNX0NDRJk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.626157Z",
     "iopub.status.busy": "2022-04-22T16:56:31.625913Z",
     "iopub.status.idle": "2022-04-22T16:56:31.632401Z",
     "shell.execute_reply": "2022-04-22T16:56:31.631597Z"
    },
    "id": "8mQrNX0NDRJk"
   },
   "outputs": [],
   "source": [
    "def create_train_test_arrays(n_past, df):\n",
    "    \"\"\"\n",
    "    Takes the data and creates trainX and trainY datasets. n_past values are used to predict the value at index n_past + 1\n",
    "    \"\"\"\n",
    "    # Prepare training and testing data\n",
    "    # Empty lists to be populated using formatted training data\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    # Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "    for i in range(n_past, len(df)):\n",
    "        trainX.append(df[i - n_past:i, 0:df.shape[1]])\n",
    "        trainY.append(df[i:i + 1])\n",
    "    trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "    print(\"Train dataset was successfully created:\")\n",
    "    print(f\"trainX shape == {trainX.shape}\")\n",
    "    print(f\"trainY shape == {trainY.shape}\")\n",
    "    return trainX, trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d9a6e-f62c-4844-ac1e-3dcf0e19b0fa",
   "metadata": {
    "id": "2f6d9a6e-f62c-4844-ac1e-3dcf0e19b0fa"
   },
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1586e77-55cb-43da-b845-3da936c7bf12",
   "metadata": {
    "id": "c1586e77-55cb-43da-b845-3da936c7bf12"
   },
   "source": [
    "To model our problem based on time series data, we need to recognize that the sequence of the data is also containing a lot of information and need to be used in the model. For this type of problem, a long short-term memory (LSTM) model is usually the best fit. With this model, the sequence data is processed in sequence and the data contained in the earlier sequence steps is kept during the whole process and has an influence on the prediction.\n",
    "\n",
    "If we look at our training, data, we recognize different magnitudes especially when we compare the prices in dollars and the volume data in pieces. Additionally, the prices of stocks are also arbitrary, depending on how many single stock represent the ownership of the full company. Therefore we normalize the dataset to remove these huge ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c89cd5-cb5e-4430-b915-36f5404cbce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:31.635616Z",
     "iopub.status.busy": "2022-04-22T16:56:31.635407Z",
     "iopub.status.idle": "2022-04-22T16:56:51.580534Z",
     "shell.execute_reply": "2022-04-22T16:56:51.579826Z"
    },
    "id": "e3c89cd5-cb5e-4430-b915-36f5404cbce4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7550e5-70bd-4f12-ba43-18873cf33685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.584371Z",
     "iopub.status.busy": "2022-04-22T16:56:51.584039Z",
     "iopub.status.idle": "2022-04-22T16:56:51.640910Z",
     "shell.execute_reply": "2022-04-22T16:56:51.640236Z"
    },
    "id": "3d7550e5-70bd-4f12-ba43-18873cf33685"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/1489511341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalize the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_for_training_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(data)\n",
    "df_for_training_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30619168-7816-4460-b0b4-43d793f77298",
   "metadata": {
    "id": "30619168-7816-4460-b0b4-43d793f77298"
   },
   "source": [
    "Additional layers -> decrease performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7Mito7xMW5dQ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.644446Z",
     "iopub.status.busy": "2022-04-22T16:56:51.644229Z",
     "iopub.status.idle": "2022-04-22T16:56:51.651345Z",
     "shell.execute_reply": "2022-04-22T16:56:51.650683Z"
    },
    "id": "7Mito7xMW5dQ"
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape, output_shape, activation=\"relu\", init_mode='uniform', learning_rate=0.001,beta_1=0.9,beta_2=0.999,amsgrad=False, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Returns a predefined model object\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, amsgrad=amsgrad)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation=activation, input_shape=(\n",
    "        input_shape[1], input_shape[2]), return_sequences=True, kernel_initializer=init_mode))\n",
    "    model.add(LSTM(32, activation=activation, return_sequences=False, kernel_initializer=init_mode))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "    # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "    model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error',metrics=[\n",
    "        metrics.RootMeanSquaredError(),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gqHalCdXEFg",
   "metadata": {
    "id": "5gqHalCdXEFg"
   },
   "source": [
    "An important decision to make is how many past data sets (each data set representing one date) are used to predict the following data set. This influences how we create our training data set. We tested out different values and the result is, that a quite good solution is 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3f5e17-664b-4ddc-8586-19f389ebda32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.654693Z",
     "iopub.status.busy": "2022-04-22T16:56:51.654361Z",
     "iopub.status.idle": "2022-04-22T16:56:51.658905Z",
     "shell.execute_reply": "2022-04-22T16:56:51.658238Z"
    },
    "id": "ab3f5e17-664b-4ddc-8586-19f389ebda32",
    "outputId": "e53a5150-b8a4-4f17-bf80-f230f0bfe8c7"
   },
   "outputs": [],
   "source": [
    "# n_past_set = [30, 60, 90, 120, 180, 200]\n",
    "# results = {}\n",
    "\n",
    "# for n_past in n_past_set:\n",
    "#     print(f\"Calculating for {n_past} ....\")\n",
    "#     # n_past = 60\n",
    "#     trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)\n",
    "\n",
    "#     def get_model(input_shape, output_shape):\n",
    "#         \"\"\"\n",
    "#         Returns a predefined model object\n",
    "#         \"\"\"\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(64, activation='relu', input_shape=(\n",
    "#             input_shape[1], input_shape[2]), return_sequences=True))\n",
    "#         model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "#         # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "#         model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "#         model.compile(optimizer='adam', loss='mse',metrics=[\n",
    "#             metrics.RootMeanSquaredError(),\n",
    "#         ])\n",
    "#         return model\n",
    "\n",
    "#     model = get_model(input_shape=trainX.shape, output_shape=trainY.shape)\n",
    "#     # model.summary()\n",
    "\n",
    "#     # fit the model\n",
    "#     history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_split=0.1, verbose=0)\n",
    "    \n",
    "#     results[n_past] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f53ac55-1a6c-419d-8d6c-88715ab44a6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.662173Z",
     "iopub.status.busy": "2022-04-22T16:56:51.661745Z",
     "iopub.status.idle": "2022-04-22T16:56:51.665120Z",
     "shell.execute_reply": "2022-04-22T16:56:51.664399Z"
    },
    "id": "0f53ac55-1a6c-419d-8d6c-88715ab44a6a",
    "outputId": "792bf232-a769-4f84-d6c8-53f24562a0b4"
   },
   "outputs": [],
   "source": [
    "# # history.history.keys()\n",
    "# f = plt.figure()\n",
    "# f.set_figwidth(15)\n",
    "# f.set_figheight(10)\n",
    "\n",
    "# for n_past in n_past_set:\n",
    "#     plt.plot(results[n_past].history['root_mean_squared_error'], label = str(n_past))\n",
    "#     plt.legend(loc=\"upper right\")\n",
    "#     plt.title('model root_mean_squared_error')\n",
    "#     plt.ylabel('value')\n",
    "#     plt.xlabel('epoch')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prVPWaauw_iU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.668389Z",
     "iopub.status.busy": "2022-04-22T16:56:51.668010Z",
     "iopub.status.idle": "2022-04-22T16:56:51.683037Z",
     "shell.execute_reply": "2022-04-22T16:56:51.682363Z"
    },
    "id": "prVPWaauw_iU",
    "outputId": "a8d85877-fc80-4d86-aebb-fee9d2937cc4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_for_training_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/513367022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_test_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_for_training_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_for_training_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "n_past = 120\n",
    "trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a0f35-cfe1-48ad-996e-67fabe8ff9ae",
   "metadata": {
    "id": "a83a0f35-cfe1-48ad-996e-67fabe8ff9ae"
   },
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "Now that we have our dataset ready, we need to find out what is a good model to predict the data set for the next price based on the last 120 data sets. After trying out several amounts of layers, we conclude that having two layers with 64 and 32 nodes leads to a quite good result. We choose the adam optimizer because it is known to be a good and efficient optimizer for regression problems.\n",
    "\n",
    "We can use GridSearchCV to further optimize our model. First, we check different combinations of batch_sizes and epochs to see which combinations leads to the best result.\n",
    "\n",
    "Furthermore, we can check different parameters for our adam activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f205a155-bf94-4389-813c-289b7fb29457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.686655Z",
     "iopub.status.busy": "2022-04-22T16:56:51.686414Z",
     "iopub.status.idle": "2022-04-22T16:56:51.690633Z",
     "shell.execute_reply": "2022-04-22T16:56:51.690019Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_gridsearch(result):\n",
    "    \"\"\"\n",
    "    Prints an formatted output for the GridSeachCV result\n",
    "    \"\"\"\n",
    "    print('Best Estimator: %s' % result.best_estimator_)\n",
    "    print('Best Hyperparameters: %s' % result.best_params_)\n",
    "    print(f'Best Score: {result.best_score_}') # score according to the metric we passed in refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b94e5b3-c9d5-4f57-b321-253e40d2d21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:51.693781Z",
     "iopub.status.busy": "2022-04-22T16:56:51.693543Z",
     "iopub.status.idle": "2022-04-22T16:56:58.153447Z",
     "shell.execute_reply": "2022-04-22T16:56:58.152741Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/2174150362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Wrap the model to use the GridSearchCV library from sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/adriangb/scikeras\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "# Wrap the model to use the GridSearchCV library from sklearn\n",
    "model = KerasRegressor(model=get_model, input_shape=trainX.shape, output_shape=trainY.shape, activation=\"relu\", init_mode='uniform', optimizer = \"adam\", dropout_rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37189b-a416-48cb-8018-d2b8b4c77b66",
   "metadata": {},
   "source": [
    "## Batch size and Epochs\n",
    "We test different values for batch size and epoch using GridSearch to find the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08d693f-58af-4440-938a-a2273cc623a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:58.157602Z",
     "iopub.status.busy": "2022-04-22T16:56:58.157378Z",
     "iopub.status.idle": "2022-04-22T16:56:59.217983Z",
     "shell.execute_reply": "2022-04-22T16:56:59.217339Z"
    },
    "id": "a08d693f-58af-4440-938a-a2273cc623a4",
    "outputId": "0b8fa5e5-1750-4643-f191-5218e7438a96",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/154379303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m GS = GridSearchCV(estimator = model,\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"neg_root_mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# With Sklearn, there is a convention that a higher score is always the better score, therefore root mean squared error is available as a negated score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"batch_size\" : [4, 8, 16, 32, 64], # smaller batch size require a longer training process, larger batch sizes require more powerfull hardware\n",
    "    \"epochs\" : [10, 25, 50, 100, 200, 250]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "GS = GridSearchCV(estimator = model,\n",
    "                param_grid = search_space,\n",
    "                scoring = \"neg_root_mean_squared_error\", # With Sklearn, there is a convention that a higher score is always the better score, therefore root mean squared error is available as a negated score\n",
    "                cv = 5,\n",
    "                refit = True,\n",
    "                n_jobs = 1, # Use all available threads\n",
    "            #    error_score=\"raise\",\n",
    "                verbose = 2)\n",
    "history = GS.fit(trainX, trainY[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d424f4-cac0-49a9-b08f-74408a039713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:59.222220Z",
     "iopub.status.busy": "2022-04-22T16:56:59.221009Z",
     "iopub.status.idle": "2022-04-22T16:56:59.238496Z",
     "shell.execute_reply": "2022-04-22T16:56:59.237736Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/2323958509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_gridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_gridsearch(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cfe09-8e65-435a-9ffd-c11da7436845",
   "metadata": {},
   "source": [
    "After running the GridSearchCV algorithm, we come to the best parameter combination:\n",
    "- batch_size: 32\n",
    "- epochs: 250\n",
    "\n",
    "After running different combinations, we also realized that increasing the amount of epochs may improve the model a bit more, but it increases computation time extensively.\n",
    "The best score for the neg_root_mean_squared_error is -0.6563984733875825, which leads to a root_mean_squared_error of 0.6563984733875825."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f2c4e-fa03-4a4d-80fa-e6b908fe999b",
   "metadata": {},
   "source": [
    "## Hyperparameters for adam optimizer\n",
    "The adam optimizer has several different parameters which we can test and combine to find a more optimal solution for our problem than with the default adam parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7adc5f3-d3ae-441c-af25-137149b1b9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:59.242371Z",
     "iopub.status.busy": "2022-04-22T16:56:59.241982Z",
     "iopub.status.idle": "2022-04-22T16:56:59.263043Z",
     "shell.execute_reply": "2022-04-22T16:56:59.262350Z"
    },
    "id": "c7adc5f3-d3ae-441c-af25-137149b1b9f7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/512742398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m GS_adam = GridSearchCV(estimator = model,\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"neg_root_mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# With Sklearn, there is a convention that a higher score is always the better score, therefore root mean squared error is available as a negated score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "adam_parameters = {\n",
    "    \"optimizer__learning_rate\" : [0.001,  0.0001, 0.01, 0.1],\n",
    "    \"optimizer__beta_1\" : [0.8, 0.9, 1.0],\n",
    "    \"optimizer__beta_2\" : [0.89, 0.99, 1.0],\n",
    "    \"optimizer__amsgrad\" : [True, False],\n",
    "    # already optimized parameters, which we like to use again for out training\n",
    "    \"batch_size\" : [32],\n",
    "    \"epochs\" : [250]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "GS_adam = GridSearchCV(estimator = model,\n",
    "                param_grid = adam_parameters,\n",
    "                scoring = \"neg_root_mean_squared_error\", # With Sklearn, there is a convention that a higher score is always the better score, therefore root mean squared error is available as a negated score\n",
    "                cv = 5,\n",
    "                refit = True,\n",
    "                n_jobs = 1, # Use all available threads\n",
    "                error_score=\"raise\",\n",
    "                verbose = 1)\n",
    "history_adam = GS_adam.fit(trainX, trainY[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d17158-3779-4ce8-a31f-9514b41048f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T16:56:59.266375Z",
     "iopub.status.busy": "2022-04-22T16:56:59.265798Z",
     "iopub.status.idle": "2022-04-22T16:56:59.280008Z",
     "shell.execute_reply": "2022-04-22T16:56:59.279396Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1654/278108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_gridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_adam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history_adam' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_gridsearch(history_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fb0fd-e4ef-42f2-bf62-8b9a9068e703",
   "metadata": {},
   "source": [
    "We come to the conclusion, that our best parameters are:\n",
    "- optimizer__amsgrad: False\n",
    "- optimizer__beta_1: 0.8\n",
    "- optimizer__beta_2: 0.89\n",
    "- optimizer__learning_rate: 0.01\n",
    "\n",
    "The best score for the neg_root_mean_squared_error is -0.8073114933087524, which leads to a root_mean_squared_error of 0.8073114933087524."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac306ab5-cce3-4d29-b476-44b509d272d1",
   "metadata": {
    "id": "ac306ab5-cce3-4d29-b476-44b509d272d1"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZxjOxSLb74d",
   "metadata": {
    "id": "IZxjOxSLb74d"
   },
   "source": [
    "After testing different LSTM networks, hyperparameters and training parameters either manually, or by the use of GridSearch, we can come to a conclusion for a quite good model:\n",
    "\n",
    "- 120 days as input data to predict for day 121\n",
    "- 2 LSTM layers with 64 and 32 nodes respectively\n",
    "- Batch size for training of 32 and 250 training epochs. More epochs improve our result, but they lead to a lot of computation time. Use less epochs may be also sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4857e-be83-4549-b2ac-72f13f1539e2",
   "metadata": {
    "id": "69b4857e-be83-4549-b2ac-72f13f1539e2"
   },
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0829794-1bab-4eeb-9ac6-5a77049e8943",
   "metadata": {},
   "source": [
    "There are possibilties to spend time and computation capacity to further improve the model. Furthermore, there are many more different optimizers as well as activation functions available, which have each individually countless different parameter combinations. These could be evaluated to see if there is a possibility to still improve the model.\n",
    "Also, other techniques except GridSearch could be used to test different sets of parameters. There is a library called RandomSearch which is similar to GridSearch, but creates more random combination by using ranges of values for the parameters.\n",
    "A different option woult be the usage of Genetic algorithms. These algorithms apply a model similar to evolution to the models to find a optimal set of paramter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd07c8-a667-44b6-bb7d-415d758031de",
   "metadata": {
    "id": "cbdd07c8-a667-44b6-bb7d-415d758031de"
   },
   "source": [
    "# Conclusion/Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a999f82-1c45-417b-bbf2-3548a4201974",
   "metadata": {
    "id": "8012306c-caeb-405e-ba24-05440d298942"
   },
   "source": [
    "Training machine learning models is a very time intensive process, if it is done wrong. Therefore I come to the following conclusions to speed up development in the future:\n",
    "- Test all scripts etc. with small amounts of data to verify that everything runs correct to avoid breakages after hours of model training and grid search\n",
    "- Run scripts overnight to not waste time waiting for the result\n",
    "- Run scripts on a server so that it can run without occuping local resources and wearing down a laptops CPU"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
