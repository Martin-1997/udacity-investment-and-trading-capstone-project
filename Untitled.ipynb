{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7917757-a0f6-4199-baca-a089a734781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 22:13:09.608919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-16 22:13:09.608939: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Import normal libraries\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import own libraries\n",
    "from data_api.init_db import return_engine\n",
    "from data_api.db import tickers_to_ticker_ids, get_all_price_data_sets, load_formatted_train_data\n",
    "from models.model_func import create_train_test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5658c21-37c7-43e1-8c18-c94e66aedcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data paths\n",
    "current_dir = os.getcwd()\n",
    "database_dir = os.path.join(current_dir, \"data\")\n",
    "db_filename = \"database.db\"\n",
    "model_dir = os.path.join(current_dir, \"data/models\")\n",
    "scaler_dir = os.path.join(current_dir, \"data/scalers\")\n",
    "# Setup the database engine\n",
    "engine = return_engine(database_dir, db_filename=db_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3b22f-cf57-4077-9125-591ce8a0175c",
   "metadata": {},
   "source": [
    "# Problem Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbca84b-5812-4e5a-b5c6-e6a70d2bbd6c",
   "metadata": {},
   "source": [
    "From the Yahoo Finance API, we get the 6 different values for each day. High price, low price, open price, close price, adjusted close price and volume. The 5 price data variables are all closely linked together, and only the adjusted close price reflects additional events like divident payouts or stock splits. Therefore, we only consider the adjusted close price (now referred to as \"price\") and the volume for our models. \n",
    "\n",
    "Our goal is to predict future prices based on the past prices and the past volume. We can use the price/volume data of multiple stocks to predict the price/volume data of those multiple stocks to make use of the information hidden in the dependence of the development in those different assets. The problem is, that we have indeed a long timerange of past data, but we do not know anything about future prices (yet). Therefore, we need to split the past data into small sections and consider the earlier part of that section as the past data and the later part as the future. Therefore we can create many training sets of past/future data combinations to use for our training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0eae-2b2c-4487-ba76-c2568f58f800",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b68e5-96b1-4b0a-a2db-137d8213bad4",
   "metadata": {},
   "source": [
    "We need to define a useful metric or multiple metrics to evaluate the performance of our model and to compare different models with different hyperparameters. The goal of our model is to predict continious values, therefore we have a regression problem. For regression problems, we can use root mean squared error (RMSE) as a metric to optimize our model for. This metric weights larger deviations from the expected results more than lower deviations, but still has a value on the same scale as the output data.\n",
    "\n",
    "- https://keras.io/api/metrics/\n",
    "- https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae66498-3d20-4eeb-92df-c39b1d0b675b",
   "metadata": {},
   "source": [
    "# EDA (Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93135ef9-7cdc-4fee-8a9b-57604ea10e19",
   "metadata": {},
   "source": [
    "When we check the data, we encounter one problem: We do not have price and volume data available for arbitrary dates for each asset. For weekends and holidays, there is usually no trading occuring on stock exchanges and therefore no price is determined. These dates without price data vary between different assets, depending on which exchanges they are listed and in which jurisdiction the exchanges are located. For some assets, like Bitcoin, there is nevertheless price data available for all days because there is less regulation about the exchanges and the cryptocurrencies can be easily traded completly digital. Additionally, there is no price data available before the asset was available. In the case of a publicly traded company, this is usually the IPO.\n",
    "To train a model with different assets, we are therefore likely facing missing values for some assets on some dates. For the first, problem, namely missing days for holidays/weekends, we have two options:\n",
    "- Drop the data for a specific date, when no price data is missing for at least one asset\n",
    "- Fill the missing prices with the average price of the days before and after\n",
    "\n",
    "In the first case, we would potentially reduce our dataset significant, in case we have many different assets from many differen exchanges/jurisdictions. Therefore we take the second option and fill up the missing prices as mentioned.\n",
    "\n",
    "To handle the second problem, namely missing asset prices because the asset was not publicly traded, it is more difficult to handle this. We can either:\n",
    "- Limit the oldest date to the date when, the last asset was available\n",
    "- Assume an average price of the asset for the time before it was available\n",
    "- Assume a price of 0 before the asset was available\n",
    "- Assume the price of the first day when the asset was available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe1e0d8-c774-4f5e-8e05-27c94a609f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=dt(2015,1,1)\n",
    "end_date=dt.today()\n",
    "tickers = [\"GOOG\", \"AAPL\", \"BTC-USD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348c54e8-b55f-46d1-af7f-c0a390343835",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_ids = tickers_to_ticker_ids(engine, tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da19c94-2e65-46a1-b1ea-34d759ef90ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_close-4</th>\n",
       "      <th>adj_close-5</th>\n",
       "      <th>adj_close-6</th>\n",
       "      <th>volume-4</th>\n",
       "      <th>volume-5</th>\n",
       "      <th>volume-6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315.032013</td>\n",
       "      <td>24.714510</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>7.860650e+06</td>\n",
       "      <td>212818400.0</td>\n",
       "      <td>1447563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274.473999</td>\n",
       "      <td>24.018259</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>4.396280e+07</td>\n",
       "      <td>257142000.0</td>\n",
       "      <td>2059840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>286.188995</td>\n",
       "      <td>24.020525</td>\n",
       "      <td>500.585632</td>\n",
       "      <td>2.324570e+07</td>\n",
       "      <td>263188400.0</td>\n",
       "      <td>2899940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294.337006</td>\n",
       "      <td>24.357342</td>\n",
       "      <td>499.727997</td>\n",
       "      <td>2.486680e+07</td>\n",
       "      <td>160423600.0</td>\n",
       "      <td>2065054.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283.348999</td>\n",
       "      <td>25.293207</td>\n",
       "      <td>501.303680</td>\n",
       "      <td>1.998250e+07</td>\n",
       "      <td>237458000.0</td>\n",
       "      <td>3353582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>45555.992188</td>\n",
       "      <td>175.059998</td>\n",
       "      <td>2821.260010</td>\n",
       "      <td>2.964060e+10</td>\n",
       "      <td>73401800.0</td>\n",
       "      <td>962800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>43206.738281</td>\n",
       "      <td>171.830002</td>\n",
       "      <td>2743.520020</td>\n",
       "      <td>3.939340e+10</td>\n",
       "      <td>89058800.0</td>\n",
       "      <td>1178700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>43503.847656</td>\n",
       "      <td>172.139999</td>\n",
       "      <td>2729.300049</td>\n",
       "      <td>2.610197e+10</td>\n",
       "      <td>77594700.0</td>\n",
       "      <td>972400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>42287.664062</td>\n",
       "      <td>170.089996</td>\n",
       "      <td>2680.209961</td>\n",
       "      <td>2.721600e+10</td>\n",
       "      <td>76515900.0</td>\n",
       "      <td>821000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>39521.902344</td>\n",
       "      <td>165.750000</td>\n",
       "      <td>2595.929932</td>\n",
       "      <td>3.394991e+10</td>\n",
       "      <td>72088900.0</td>\n",
       "      <td>1206100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1832 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adj_close-4  adj_close-5  adj_close-6      volume-4     volume-5  \\\n",
       "0       315.032013    24.714510   523.373108  7.860650e+06  212818400.0   \n",
       "1       274.473999    24.018259   512.463013  4.396280e+07  257142000.0   \n",
       "2       286.188995    24.020525   500.585632  2.324570e+07  263188400.0   \n",
       "3       294.337006    24.357342   499.727997  2.486680e+07  160423600.0   \n",
       "4       283.348999    25.293207   501.303680  1.998250e+07  237458000.0   \n",
       "...            ...          ...          ...           ...          ...   \n",
       "1827  45555.992188   175.059998  2821.260010  2.964060e+10   73401800.0   \n",
       "1828  43206.738281   171.830002  2743.520020  3.939340e+10   89058800.0   \n",
       "1829  43503.847656   172.139999  2729.300049  2.610197e+10   77594700.0   \n",
       "1830  42287.664062   170.089996  2680.209961  2.721600e+10   76515900.0   \n",
       "1831  39521.902344   165.750000  2595.929932  3.394991e+10   72088900.0   \n",
       "\n",
       "       volume-6  \n",
       "0     1447563.0  \n",
       "1     2059840.0  \n",
       "2     2899940.0  \n",
       "3     2065054.0  \n",
       "4     3353582.0  \n",
       "...         ...  \n",
       "1827   962800.0  \n",
       "1828  1178700.0  \n",
       "1829   972400.0  \n",
       "1830   821000.0  \n",
       "1831  1206100.0  \n",
       "\n",
       "[1832 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = load_formatted_train_data(engine, tickers_ids, start_date, end_date, return_timestamp=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ca73d-4b8d-4ae6-86d3-13d0207e1077",
   "metadata": {},
   "source": [
    "## Only for  Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff96475d-bdcc-41a3-baf2-536d013d67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_big.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d9a6e-f62c-4844-ac1e-3dcf0e19b0fa",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1586e77-55cb-43da-b845-3da936c7bf12",
   "metadata": {},
   "source": [
    "To model our problem based on time series data, we need to recognize that the sequence of the data is also containing a lot of information and need to be used in the model. For this type of problem, a long short-term memory (LSTM) model is usually the best fit. With this model, the sequence data is processed in sequence and the data contained in the earlier sequence steps is kept during the whole process and has an influence on the prediction.\n",
    "\n",
    "If we look at our training, data, we recognize different magnitudes especially when we compare the prices in dollars and the volume data in pieces. Additionally, the prices of stocks are also arbitrary, depending on how many single stock represent the ownership of the full company. Therefore we normalize the dataset to remove these huge ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c89cd5-cb5e-4430-b915-36f5404cbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7550e5-70bd-4f12-ba43-18873cf33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(data)\n",
    "df_for_training_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1920435e-1680-427f-a33c-b48bd0d4aa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.72295957, -0.86920844, -1.13500987, -0.76567872,  1.10717849,\n",
       "        -0.26978645],\n",
       "       [-0.72539304, -0.88463623, -1.15137988, -0.76397896,  1.74889781,\n",
       "         0.48835554],\n",
       "       [-0.72469014, -0.88458602, -1.16920124, -0.76495436,  1.83643789,\n",
       "         1.52859564],\n",
       "       ...,\n",
       "       [ 1.86835869,  2.39750192,  2.17486378,  0.46287918, -0.85059705,\n",
       "        -0.85814927],\n",
       "       [ 1.79538797,  2.35207717,  2.10120677,  0.51532936, -0.86621597,\n",
       "        -1.04561784],\n",
       "       [ 1.62944295,  2.25590987,  1.97474916,  0.83237432, -0.9303103 ,\n",
       "        -0.56877406]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30619168-7816-4460-b0b4-43d793f77298",
   "metadata": {},
   "source": [
    "Additional layers -> decrease performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd51fbd2-68df-4daf-9ea4-4e71603bb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8, 16, 32, 64, 128] # smaller batch size require a longer training process, larger batch sizes require more powerfull hardware\n",
    "n_past_set = [30, 60, 90, 120, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3f5e17-664b-4ddc-8586-19f389ebda32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for 30 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (1802, 30, 6)\n",
      "trainY shape == (1802, 1, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 22:02:40.531951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-16 22:02:40.531972: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-16 22:02:40.531989: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-ThinkPad-T490): /proc/driver/nvidia/version does not exist\n",
      "2022-04-16 22:02:40.532195: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for 60 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (1772, 60, 6)\n",
      "trainY shape == (1772, 1, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for n_past in n_past_set:\n",
    "    print(f\"Calculating for {n_past} ....\")\n",
    "    # n_past = 60\n",
    "    trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)\n",
    "\n",
    "    def get_model(input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        Returns a predefined model object\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', input_shape=(\n",
    "            input_shape[1], input_shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "        # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "        model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "        model.compile(optimizer='adam', loss='mse',metrics=[\n",
    "            metrics.RootMeanSquaredError(),\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    model = get_model(input_shape=trainX.shape, output_shape=trainY.shape)\n",
    "    # model.summary()\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(trainX, trainY, epochs=400, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    results[n_past] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53ac55-1a6c-419d-8d6c-88715ab44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af200e-7ddf-401b-859c-84cc036b5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(15)\n",
    "f.set_figheight(10)\n",
    "\n",
    "for n_past in n_past_set:\n",
    "    plt.plot(results[n_past].history['root_mean_squared_error'], label = str(n_past))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('model root_mean_squared_error')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a0f35-cfe1-48ad-996e-67fabe8ff9ae",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757273a-fcba-4a1d-89b6-fa518ef29e8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- Losses\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "- Metrics\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d89703-2a98-4082-9a8f-ea5f5860b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space  {\n",
    "    \"optimizer\" : keras.optimizers.SGD(lr=0.01,momentum=0.9), 'adam',\n",
    "    \"loss\" : 'mean_squared_error'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d693f-58af-4440-938a-a2273cc623a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# make a GridSearchCV object\n",
    "GS = GridSearchCV(estimator = xgb_model,\n",
    "                  param_grid = search_space,\n",
    "                  scoring = [\"r2\", \"neg_root_mean_squared_error\"], #sklearn.metrics.SCORERS.keys()\n",
    "                  refit = \"r2\",\n",
    "                  cv = 5,\n",
    "                  verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0aabb-dd4b-42a2-881d-ede8078c8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea35597-f6da-4961-84b1-1b7a3c8056c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_estimator_) # to get the complete details of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecbcbb-85ce-47bc-9498-bcaef7b8be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_params_) # to get only the best hyperparameter values that we searched for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4d5d0-20be-4bb6-8b92-b6c7dafa463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_score_) # score according to the metric we passed in refit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d1ec3-d777-45a7-a34d-71e7dca39901",
   "metadata": {},
   "source": [
    "small batch size (1) is much better than larger ones, even though the training takes longer -> ~ 0.5\n",
    "validation_split 0.2 -> does not have big impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "034d8274-d806-4111-9e44-779e0a94193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset was successfully created:\n",
      "trainX shape == (1047, 30, 6)\n",
      "trainY shape == (1047, 1, 6)\n",
      "Epoch 1/20\n",
      "837/837 [==============================] - 7s 7ms/step - loss: 1.4494 - root_mean_squared_error: 1.2039 - val_loss: 1.0991 - val_root_mean_squared_error: 1.0484\n",
      "Epoch 2/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.6132 - root_mean_squared_error: 0.7830 - val_loss: 635.8591 - val_root_mean_squared_error: 25.2162\n",
      "Epoch 3/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.4281 - root_mean_squared_error: 0.6543 - val_loss: 2.1492 - val_root_mean_squared_error: 1.4660\n",
      "Epoch 4/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3734 - root_mean_squared_error: 0.6110 - val_loss: 1.1471 - val_root_mean_squared_error: 1.0710\n",
      "Epoch 5/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3494 - root_mean_squared_error: 0.5911 - val_loss: 0.7674 - val_root_mean_squared_error: 0.8760\n",
      "Epoch 6/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3398 - root_mean_squared_error: 0.5829 - val_loss: 0.5514 - val_root_mean_squared_error: 0.7425\n",
      "Epoch 7/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3178 - root_mean_squared_error: 0.5637 - val_loss: 0.6954 - val_root_mean_squared_error: 0.8339\n",
      "Epoch 8/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3219 - root_mean_squared_error: 0.5674 - val_loss: 0.8675 - val_root_mean_squared_error: 0.9314\n",
      "Epoch 9/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3175 - root_mean_squared_error: 0.5635 - val_loss: 0.7188 - val_root_mean_squared_error: 0.8478\n",
      "Epoch 10/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3151 - root_mean_squared_error: 0.5613 - val_loss: 0.6032 - val_root_mean_squared_error: 0.7767\n",
      "Epoch 11/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3126 - root_mean_squared_error: 0.5591 - val_loss: 0.6360 - val_root_mean_squared_error: 0.7975\n",
      "Epoch 12/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3070 - root_mean_squared_error: 0.5540 - val_loss: 0.3205 - val_root_mean_squared_error: 0.5661\n",
      "Epoch 13/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2997 - root_mean_squared_error: 0.5475 - val_loss: 0.6812 - val_root_mean_squared_error: 0.8254\n",
      "Epoch 14/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3024 - root_mean_squared_error: 0.5499 - val_loss: 0.2839 - val_root_mean_squared_error: 0.5328\n",
      "Epoch 15/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2959 - root_mean_squared_error: 0.5440 - val_loss: 0.3176 - val_root_mean_squared_error: 0.5636\n",
      "Epoch 16/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2978 - root_mean_squared_error: 0.5457 - val_loss: 0.3421 - val_root_mean_squared_error: 0.5849\n",
      "Epoch 17/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2925 - root_mean_squared_error: 0.5408 - val_loss: 0.3558 - val_root_mean_squared_error: 0.5965\n",
      "Epoch 18/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2962 - root_mean_squared_error: 0.5443 - val_loss: 0.3673 - val_root_mean_squared_error: 0.6060\n",
      "Epoch 19/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2886 - root_mean_squared_error: 0.5372 - val_loss: 0.2968 - val_root_mean_squared_error: 0.5448\n",
      "Epoch 20/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2901 - root_mean_squared_error: 0.5386 - val_loss: 0.3053 - val_root_mean_squared_error: 0.5525\n"
     ]
    }
   ],
   "source": [
    "n_past = 30\n",
    "trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)\n",
    "\n",
    "def get_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Returns a predefined model object\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=(\n",
    "        input_shape[1], input_shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "    # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "    model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=[\n",
    "        metrics.RootMeanSquaredError(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = get_model(input_shape=trainX.shape, output_shape=trainY.shape)\n",
    "# model.summary()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=20, batch_size=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adc5f3-d3ae-441c-af25-137149b1b9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac306ab5-cce3-4d29-b476-44b509d272d1",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4857e-be83-4549-b2ac-72f13f1539e2",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd07c8-a667-44b6-bb7d-415d758031de",
   "metadata": {},
   "source": [
    "# Conclusion/Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012306c-caeb-405e-ba24-05440d298942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
