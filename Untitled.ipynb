{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7917757-a0f6-4199-baca-a089a734781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normal libraries\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import own libraries\n",
    "from data_api.init_db import return_engine\n",
    "from data_api.db import tickers_to_ticker_ids, get_all_price_data_sets, load_formatted_train_data\n",
    "from models.model_func import create_train_test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5658c21-37c7-43e1-8c18-c94e66aedcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the data\n",
    "\n",
    "# Define the data paths\n",
    "current_dir = os.getcwd()\n",
    "database_dir = os.path.join(current_dir, \"data\")\n",
    "db_filename = \"database.db\"\n",
    "model_dir = os.path.join(current_dir, \"data/models\")\n",
    "scaler_dir = os.path.join(current_dir, \"data/scalers\")\n",
    "# Setup the database engine\n",
    "engine = return_engine(database_dir, db_filename=db_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3b22f-cf57-4077-9125-591ce8a0175c",
   "metadata": {},
   "source": [
    "# Problem Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbca84b-5812-4e5a-b5c6-e6a70d2bbd6c",
   "metadata": {},
   "source": [
    "From the Yahoo Finance API, we get the 6 different values for each day. High price, low price, open price, close price, adjusted close price and volume. The 5 price data variables are all closely linked together, and only the adjusted close price reflects additional events like divident payouts or stock splits. Therefore, we only consider the adjusted close price (now referred to as \"price\") and the volume for our models. \n",
    "\n",
    "Our goal is to predict future prices based on the past prices and the past volume. We can use the price/volume data of multiple stocks to predict the price/volume data of those multiple stocks to make use of the information hidden in the dependence of the development in those different assets. The problem is, that we have indeed a long timerange of past data, but we do not know anything about future prices (yet). Therefore, we need to split the past data into small sections and consider the earlier part of that section as the past data and the later part as the future. Therefore we can create many training sets of past/future data combinations to use for our training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0eae-2b2c-4487-ba76-c2568f58f800",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b68e5-96b1-4b0a-a2db-137d8213bad4",
   "metadata": {},
   "source": [
    "We need to define a useful metric or multiple metrics to evaluate the performance of our model and to compare different models with different hyperparameters. The goal of our model is to predict continious values, therefore we have a regression problem. For regression problems, we can use root mean squared error (RMSE) as a metric to optimize our model for. This metric weights larger deviations from the expected results more than lower deviations, but still has a value on the same scale as the output data.\n",
    "\n",
    "- https://keras.io/api/metrics/\n",
    "- https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae66498-3d20-4eeb-92df-c39b1d0b675b",
   "metadata": {},
   "source": [
    "# EDA (Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93135ef9-7cdc-4fee-8a9b-57604ea10e19",
   "metadata": {},
   "source": [
    "When we check the data, we encounter one problem: We do not have price and volume data available for arbitrary dates for each asset. For weekends and holidays, there is usually no trading occuring on stock exchanges and therefore no price is determined. These dates without price data vary between different assets, depending on which exchanges they are listed and in which jurisdiction the exchanges are located. For some assets, like Bitcoin, there is nevertheless price data available for all days because there is less regulation about the exchanges and the cryptocurrencies can be easily traded completly digital. Additionally, there is no price data available before the asset was available. In the case of a publicly traded company, this is usually the IPO.\n",
    "To train a model with different assets, we are therefore likely facing missing values for some assets on some dates. For the first, problem, namely missing days for holidays/weekends, we have two options:\n",
    "- Drop the data for a specific date, when no price data is missing for at least one asset\n",
    "- Fill the missing prices with the average price of the days before and after\n",
    "\n",
    "In the first case, we would potentially reduce our dataset significant, in case we have many different assets from many differen exchanges/jurisdictions. Therefore we take the second option and fill up the missing prices as mentioned.\n",
    "\n",
    "To handle the second problem, namely missing asset prices because the asset was not publicly traded, it is more difficult to handle this. We can either:\n",
    "- Limit the oldest date to the date when, the last asset was available\n",
    "- Assume an average price of the asset for the time before it was available\n",
    "- Assume a price of 0 before the asset was available\n",
    "- Assume the price of the first day when the asset was available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bfe1e0d8-c774-4f5e-8e05-27c94a609f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=dt(2015,1,1)\n",
    "end_date=dt.today()\n",
    "tickers = [\"GOOG\", \"AAPL\", \"BTC-USD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "348c54e8-b55f-46d1-af7f-c0a390343835",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_ids = tickers_to_ticker_ids(engine, tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5da19c94-2e65-46a1-b1ea-34d759ef90ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_close-4</th>\n",
       "      <th>adj_close-5</th>\n",
       "      <th>adj_close-6</th>\n",
       "      <th>volume-4</th>\n",
       "      <th>volume-5</th>\n",
       "      <th>volume-6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314.248993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.036550e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315.032013</td>\n",
       "      <td>24.714510</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>7.860650e+06</td>\n",
       "      <td>212818400.0</td>\n",
       "      <td>1447563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281.082001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.305440e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264.195007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.562910e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>274.473999</td>\n",
       "      <td>24.018259</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>4.396280e+07</td>\n",
       "      <td>257142000.0</td>\n",
       "      <td>2059840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>42287.664062</td>\n",
       "      <td>170.089996</td>\n",
       "      <td>2680.209961</td>\n",
       "      <td>2.721600e+10</td>\n",
       "      <td>76515900.0</td>\n",
       "      <td>821000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>42782.136719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.605077e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>42207.671875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.765448e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>39521.902344</td>\n",
       "      <td>165.750000</td>\n",
       "      <td>2595.929932</td>\n",
       "      <td>3.394991e+10</td>\n",
       "      <td>72088900.0</td>\n",
       "      <td>1206100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>40198.968750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.668012e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2659 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adj_close-4  adj_close-5  adj_close-6      volume-4     volume-5  \\\n",
       "1       314.248993          NaN          NaN  8.036550e+06          NaN   \n",
       "2       315.032013    24.714510   523.373108  7.860650e+06  212818400.0   \n",
       "3       281.082001          NaN          NaN  3.305440e+07          NaN   \n",
       "4       264.195007          NaN          NaN  5.562910e+07          NaN   \n",
       "5       274.473999    24.018259   512.463013  4.396280e+07  257142000.0   \n",
       "...            ...          ...          ...           ...          ...   \n",
       "2655  42287.664062   170.089996  2680.209961  2.721600e+10   76515900.0   \n",
       "2656  42782.136719          NaN          NaN  1.605077e+10          NaN   \n",
       "2657  42207.671875          NaN          NaN  1.765448e+10          NaN   \n",
       "2658  39521.902344   165.750000  2595.929932  3.394991e+10   72088900.0   \n",
       "2659  40198.968750          NaN          NaN  3.668012e+10          NaN   \n",
       "\n",
       "       volume-6  \n",
       "1           NaN  \n",
       "2     1447563.0  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "5     2059840.0  \n",
       "...         ...  \n",
       "2655   821000.0  \n",
       "2656        NaN  \n",
       "2657        NaN  \n",
       "2658  1206100.0  \n",
       "2659        NaN  \n",
       "\n",
       "[2659 rows x 6 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = load_formatted_train_data(engine, tickers_ids, start_date, end_date, return_timestamp=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff96475d-bdcc-41a3-baf2-536d013d67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"data_big.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d9a6e-f62c-4844-ac1e-3dcf0e19b0fa",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1586e77-55cb-43da-b845-3da936c7bf12",
   "metadata": {},
   "source": [
    "To model our problem based on time series data, we need to recognize that the sequence of the data is also containing a lot of information and need to be used in the model. For this type of problem, a long short-term memory (LSTM) model is usually the best fit. With this model, the sequence data is processed in sequence and the data contained in the earlier sequence steps is kept during the whole process and has an influence on the prediction.\n",
    "\n",
    "If we look at our training, data, we recognize different magnitudes especially when we compare the prices in dollars and the volume data in pieces. Additionally, the prices of stocks are also arbitrary, depending on how many single stock represent the ownership of the full company. Therefore we normalize the dataset to remove these huge ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3c89cd5-cb5e-4430-b915-36f5404cbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d7550e5-70bd-4f12-ba43-18873cf33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(data)\n",
    "df_for_training_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1920435e-1680-427f-a33c-b48bd0d4aa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27147537, -1.05588872, -0.90664004, -0.46702343, -0.34953918,\n",
       "        -0.48594865],\n",
       "       [-0.25940268, -1.0560487 , -0.87942634, -0.46586821, -0.0714401 ,\n",
       "        -0.20929733],\n",
       "       [-0.23744133, -1.0517759 , -0.87332343, -0.24161438, -0.56862819,\n",
       "        -0.82063067],\n",
       "       ...,\n",
       "       [ 1.30154258,  1.87426609,  1.6844244 , -0.04441577, -0.7817246 ,\n",
       "        -0.86688286],\n",
       "       [ 1.2344682 ,  1.82841394,  1.60799853,  0.00645137, -0.80065527,\n",
       "        -1.08435436],\n",
       "       [ 1.08193222,  1.73134182,  1.47678722,  0.31392734, -0.87833981,\n",
       "        -0.53119536]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30619168-7816-4460-b0b4-43d793f77298",
   "metadata": {},
   "source": [
    "Additional layers -> decrease performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6657c-fff5-4a0f-8475-56c5358f924f",
   "metadata": {},
   "source": [
    "- Optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- Losses\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "- Metrics\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab3f5e17-664b-4ddc-8586-19f389ebda32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for 30 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (1047, 30, 6)\n",
      "trainY shape == (1047, 1, 6)\n",
      "Calculating for 60 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (1017, 60, 6)\n",
      "trainY shape == (1017, 1, 6)\n",
      "Calculating for 90 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (987, 90, 6)\n",
      "trainY shape == (987, 1, 6)\n",
      "Calculating for 120 ....\n",
      "Train dataset was successfully created:\n",
      "trainX shape == (957, 120, 6)\n",
      "trainY shape == (957, 1, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(input_shape\u001b[38;5;241m=\u001b[39mtrainX\u001b[38;5;241m.\u001b[39mshape, output_shape\u001b[38;5;241m=\u001b[39mtrainY\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# model.summary()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m results[n_past] \u001b[38;5;241m=\u001b[39m history\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/stock_price_pred/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "n_past_set = [30, 60, 90, 120, 200, 300]\n",
    "\n",
    "for n_past in n_past_set:\n",
    "    print(f\"Calculating for {n_past} ....\")\n",
    "    # n_past = 60\n",
    "    trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)\n",
    "\n",
    "    def get_model(input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        Returns a predefined model object\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', input_shape=(\n",
    "            input_shape[1], input_shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "        # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "        model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "        model.compile(optimizer='adam', loss='mse',metrics=[\n",
    "            metrics.RootMeanSquaredError(),\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    model = get_model(input_shape=trainX.shape, output_shape=trainY.shape)\n",
    "    # model.summary()\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(trainX, trainY, epochs=100, batch_size=1, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    results[n_past] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53ac55-1a6c-419d-8d6c-88715ab44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af200e-7ddf-401b-859c-84cc036b5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(15)\n",
    "f.set_figheight(10)\n",
    "\n",
    "for n_past in n_past_set:\n",
    "    plt.plot(results[n_past].history['root_mean_squared_error'], label = str(n_past))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('model root_mean_squared_error')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a0f35-cfe1-48ad-996e-67fabe8ff9ae",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d89703-2a98-4082-9a8f-ea5f5860b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space  {\n",
    "    optimizer = keras.optimizers.SGD(lr=0.01,momentum=0.9), 'adam',\n",
    "    loss='mean_squared_error'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d1ec3-d777-45a7-a34d-71e7dca39901",
   "metadata": {},
   "source": [
    "small batch size (1) is much better than larger ones, even though the training takes longer -> ~ 0.5\n",
    "validation_split 0.2 -> does not have big impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "034d8274-d806-4111-9e44-779e0a94193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset was successfully created:\n",
      "trainX shape == (1047, 30, 6)\n",
      "trainY shape == (1047, 1, 6)\n",
      "Epoch 1/20\n",
      "837/837 [==============================] - 7s 7ms/step - loss: 1.4494 - root_mean_squared_error: 1.2039 - val_loss: 1.0991 - val_root_mean_squared_error: 1.0484\n",
      "Epoch 2/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.6132 - root_mean_squared_error: 0.7830 - val_loss: 635.8591 - val_root_mean_squared_error: 25.2162\n",
      "Epoch 3/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.4281 - root_mean_squared_error: 0.6543 - val_loss: 2.1492 - val_root_mean_squared_error: 1.4660\n",
      "Epoch 4/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3734 - root_mean_squared_error: 0.6110 - val_loss: 1.1471 - val_root_mean_squared_error: 1.0710\n",
      "Epoch 5/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3494 - root_mean_squared_error: 0.5911 - val_loss: 0.7674 - val_root_mean_squared_error: 0.8760\n",
      "Epoch 6/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3398 - root_mean_squared_error: 0.5829 - val_loss: 0.5514 - val_root_mean_squared_error: 0.7425\n",
      "Epoch 7/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3178 - root_mean_squared_error: 0.5637 - val_loss: 0.6954 - val_root_mean_squared_error: 0.8339\n",
      "Epoch 8/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3219 - root_mean_squared_error: 0.5674 - val_loss: 0.8675 - val_root_mean_squared_error: 0.9314\n",
      "Epoch 9/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3175 - root_mean_squared_error: 0.5635 - val_loss: 0.7188 - val_root_mean_squared_error: 0.8478\n",
      "Epoch 10/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3151 - root_mean_squared_error: 0.5613 - val_loss: 0.6032 - val_root_mean_squared_error: 0.7767\n",
      "Epoch 11/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3126 - root_mean_squared_error: 0.5591 - val_loss: 0.6360 - val_root_mean_squared_error: 0.7975\n",
      "Epoch 12/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3070 - root_mean_squared_error: 0.5540 - val_loss: 0.3205 - val_root_mean_squared_error: 0.5661\n",
      "Epoch 13/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2997 - root_mean_squared_error: 0.5475 - val_loss: 0.6812 - val_root_mean_squared_error: 0.8254\n",
      "Epoch 14/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.3024 - root_mean_squared_error: 0.5499 - val_loss: 0.2839 - val_root_mean_squared_error: 0.5328\n",
      "Epoch 15/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2959 - root_mean_squared_error: 0.5440 - val_loss: 0.3176 - val_root_mean_squared_error: 0.5636\n",
      "Epoch 16/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2978 - root_mean_squared_error: 0.5457 - val_loss: 0.3421 - val_root_mean_squared_error: 0.5849\n",
      "Epoch 17/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2925 - root_mean_squared_error: 0.5408 - val_loss: 0.3558 - val_root_mean_squared_error: 0.5965\n",
      "Epoch 18/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2962 - root_mean_squared_error: 0.5443 - val_loss: 0.3673 - val_root_mean_squared_error: 0.6060\n",
      "Epoch 19/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2886 - root_mean_squared_error: 0.5372 - val_loss: 0.2968 - val_root_mean_squared_error: 0.5448\n",
      "Epoch 20/20\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 0.2901 - root_mean_squared_error: 0.5386 - val_loss: 0.3053 - val_root_mean_squared_error: 0.5525\n"
     ]
    }
   ],
   "source": [
    "n_past = 30\n",
    "trainX, trainY = create_train_test_arrays(n_past=n_past, df=df_for_training_scaled)\n",
    "\n",
    "def get_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Returns a predefined model object\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=(\n",
    "        input_shape[1], input_shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    # If non-negative return values are required, this should be accomplished by the layers in the network. Anyway, with only non-negative input values, negative output values are very unlikely.\n",
    "    # The relu activation function only returns positive values. -> This returns the same values for each predicted date\n",
    "    model.add(Dense(output_shape[2]))  # ,  W_constraint=nonneg()))\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=[\n",
    "        metrics.RootMeanSquaredError(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = get_model(input_shape=trainX.shape, output_shape=trainY.shape)\n",
    "# model.summary()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=20, batch_size=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adc5f3-d3ae-441c-af25-137149b1b9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac306ab5-cce3-4d29-b476-44b509d272d1",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4857e-be83-4549-b2ac-72f13f1539e2",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd07c8-a667-44b6-bb7d-415d758031de",
   "metadata": {},
   "source": [
    "# Conclusion/Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012306c-caeb-405e-ba24-05440d298942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
